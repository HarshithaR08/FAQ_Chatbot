# ğŸ§  MSU FAQ Chatbot â€” Retrieval-Augmented Answering System for University Resources

**Goal:** Build an AI-powered FAQ chatbot for Montclair State University that answers student questions by grounding responses in official university documents (Registrar, Policies, Global Engagement, SoC, etc.).

## ğŸš€ Key Features

- **Automated Web Data Ingestion**
  - Crawls and fetches sitemap + manually added URLs from Montclairâ€™s official sites.
  - Handles PDFs and HTML content uniformly.
  - Incrementally updates when pages change (using `Last-Modified`, `ETag`, or hash checks).

- **Document Cleaning & Structuring**
  - Converts HTML and PDFs into clean Markdown.
  - Splits long content into small, semantically meaningful chunks (RAG-ready).

- **Semantic Retrieval Engine**
  - Uses `sentence-transformers/all-MiniLM-L6-v2` to embed chunks.
  - Stores vectors in **ChromaDB** for fast, semantic similarity search.

- **RAG (Retrieval-Augmented Generation) Architecture**
  - Retrieves the most relevant policy passages.
  - Supplies them as context to an LLM (e.g., Mistral-7B or LLaMA 2-Chat) for accurate, grounded answers.

- **Extensible Design**
  - Supports new departments, sites, or changed URLs via YAML configuration.
  - Modular structure allows plugging in different embedding or LLM models later.

---

## ğŸ—ï¸ System Architecture Overview

```text
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Student Question  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Embedding Model    â”‚ (MiniLM-L6-v2)
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ vector
                             â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Vector DB (ChromaDB)     â”‚
                â”‚ Stores all document chunksâ”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ top-k results
                             â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ LLM (Mistral/LLaMA)      â”‚
                â”‚ Generates grounded answerâ”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
---

## ğŸ§© Repository Structure
```text

faq_chatbot/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                                         # Raw HTML/PDF fetched from sites (ignored in git)
â”‚   â”œâ”€â”€ processed/                                   # Cleaned Markdown (ignored in git)
â”‚   â””â”€â”€ url_list.csv                                 # Master list of discovered URLs (ignored in git)
â”‚
â”œâ”€â”€ ingest/
â”‚   â”œâ”€â”€ fetch_from_sitemap.py                        # Crawl + filter sitemap URLs
â”‚   â”œâ”€â”€ discovery_links.py                           # Discover deep links and adapt to new sites
â”‚   â”œâ”€â”€ pdf_to_markdown.py                           # Convert policy PDFs to text
â”‚   â”œâ”€â”€ html_to_markdown.py                          # HTML â†’ Markdown
â”‚   â”œâ”€â”€ extract_tables.py                            # Extract tables into structured JSON/Markdown
â”‚   â”œâ”€â”€ render_fetch.py                              # Rendered fetch (for JS-heavy pages, future)
â”‚   â”œâ”€â”€ run_all.py                                   # End-to-end ingestion & conversion pipeline
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ kb/
â”‚   â”œâ”€â”€ build_kb.py                                  # Generate Q/A pairs for FAQs
â”‚   â”œâ”€â”€ faq.csv                                      # Seed FAQ questions (curated)
â”‚   â”œâ”€â”€ faq.jsonl                                    # JSONL version of FAQ data
â”‚   â”œâ”€â”€ seed_questions.yaml                          # Starter questions for evaluation
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ chunker.py                                   # Split Markdown into smaller sections
â”‚   â”œâ”€â”€ chunks.jsonl                                 # Chunked text ready for embeddings (ignored in git for new files)
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ vectorstore/
â”‚   â”œâ”€â”€ build_index_retrieval.py                      # Build Chroma index + retrieval pipeline
â”‚   â”œâ”€â”€ rerank.py                                     # Intent detection + reranking logic
â”‚   â”œâ”€â”€ test_retrieval_query.py                       # Manual test harness for retrieval
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â””â”€â”€ regression_testing_retrieval.py           # Automated regression tests
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ eval/
â”‚   â”œâ”€â”€ policy_postfilter.py                          # Keep only relevant policy topics
â”‚   â”œâ”€â”€ preview_chunks.py                             # Preview sample chunks for sanity checks
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ sources.yaml                                      # URL/bucket configuration for ingestion
â”œâ”€â”€ SECURITY.md                                       # Security & commit guidelines
â”œâ”€â”€ README.md                                         # Project overview and usage
â””â”€â”€ requirements.txt                                  # Required dependencies
```

---

## âš™ï¸ Prerequisites

Python â‰¥ 3.9
Virtual environment (venv or conda)
Dependencies: 
```bash
pip install -r requirements.txt
```
---

## ğŸ§° Quick Start

```bash
# 1. Clone repository
git clone https://github.com/<your_username>/faq_chatbot.git
cd faq_chatbot

# 2. Activate virtual environment
python -m venv .venv
source .venv/bin/activate                               # Mac/Linux
.venv\Scripts\activate                                  # Windows

# 3. Install dependencies
pip install -r requirements.txt

# 4. Run the pipeline
python -m ingest.run_all                                # Crawl + HTML â†’ Markdown
python -m ingest.pdf_to_markdown                        # Extract PDFs â†’ Markdown (if any)
python -m eval.policy_postfilter                        # Filter / focus on relevant policies
python -m rag.chunker                                   # Split Markdown into chunks
python -m eval.preview_chunks                           # Optional: sanity-check chunks
python -m kb.build_kb                                   # Optional: generate FAQ-style Q/A
python -m vectorstore.build_index_retrieval
python -m vectorstore.test_retrieval_query
python -m vectorstore.tests.regression_testing_retrieval

```

---

## ğŸ§  Configuration

- **Main configuration lives in sources.yaml:**
  - Add new sitemap URLs or manual pages here.
  - Edit regex rules under `buckets` to control which URLs are included/excluded.
  - The system automatically adapts to new external sites using discovery + auto-sitemap detection.

---

## ğŸ§© Deployment Options (Future)

- Local FastAPI server (for REST API)
- Streamlit Web UI (for chatbot interface)
- Cloud deployment via:
  - Docker container on Azure App Service or AWS Lightsail
  - Optional GPU inference with Hugging Face Inference API

---

## ğŸ’» Development Environment Setup

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```
 
- **Folder paths (inside YAML):**
  - data/raw â†’ stores raw HTML/PDFs
  - data/processed â†’ cleaned Markdown
  - rag/chunks.jsonl â†’ chunked knowledge base
  - vectorstore/chroma/ â†’ persistent ChromaDB index

---

## ğŸ¤ Contribution Guidelines

- 1. Fork this repository.
- 2. Create a new branch:
```bash
     git checkout -b feature/my-feature
```
- 3. Commit your changes:
```bash
     git commit -m "Added new sitemap or fixed bug"
```
- 4. Push the branch and open a Pull Request.

---

## ğŸ”’ Security & Commit Policy

ğŸ“‚ See [`SECURITY.md`](./SECURITY.md)

 - Do not commit API keys, credentials, or .env files.
 - .gitignore includes common sensitive paths (/data/, .venv/, /state/).
 - Never push raw student or internal documents.
 - All scraping follows robots.txt and university data-access policies.

---

## ğŸ™ Acknowledgments

- Montclair State University â€” School of Computing
- Faculty Advisors: Prof. Shang, Prof. Wang
- Contributors: Harshitha Ramakrishna, Imaduddin Syed, Lam Nguyen, Prudhvi Raj Kore
- Technologies: Python, ChromaDB, Sentence-Transformers, Mistral-7B, Streamlit

---

**ğŸ“˜ This repository is part of the MSU AI FAQ Chatbot project â€” a prototype system integrating RAG pipelines for student support and policy retrieval.**
